{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
       "include_colab_link": true,
        "name": "05-11-21-python-compvs-edabit.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "euRda90J4Fcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8081/"
        },
        "id": "X8BzVrZU3enL",
        "outputId": "3937f7b3-7b12-416c-8185-035337ce51db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
                "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir  = '/content/drive/compVisionPackage/Agricultural-crops'\n",
        "os.chdir(base_dir)\n",
        "\n",
        "# list each directory name with # (label name)\n",
        "drive_list = tf.io.gfile.listdir(base_dir)\n",
        "\n",
        
        "# display number of labels\n",
        "drive_labels = len(drive_list)\n",
        "print(f\"Total labels: = {drive_labels}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8085/"
        },
        "id": "R7oR6RUO4Kyw",
        "outputId": "fb230141-261e-4c7e-a8b0-aa96d2d2de0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Class Labels = 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversion of images into NumPy arrays\n",
        "def convert_jpeg_to_csv(img):\n",
        "  arr = np.asarray(img)\n",
        "  print(arr.shape)\n",
        
        
        "# Make a 2D list of lists from a 3D array\n",
        "  lst = []\n",
        "  for row in arr:\n",
        "    tmp = []\n",
        "    for col in row:\n",
        "        tmp.append(str(col))\n",
        "    lst.append(tmp)\n",
        
        
        "#Copy a list to a CSV file.\n",
        "  with open('my_file.csv', 'w') as f:\n",
        "    for row in lst:\n",
        "        f.write(','.join(row) + '\\n')"
      ],
      "metadata": {
        "id": "4RfPFcoFgX1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svm(W, X, Y):\n",
        "    reg_strength = 10000  # regularization strength\n",
        "\n",
        "    #  compute svm\n",
        "    N = X.shape[0]\n",
        "    # np.dot(X, W) product of two arrays\n",
        "    distances = 1 - Y * (np.dot(X, W))\n",
        "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
        "    hinge_loss = reg_strength * (np.sum(distances) / N)\n",
        "\n",
        
        
        "    # determine costs\n",
        "    # np.dot(W, W) product of two arrays\n",
        "    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
        "    return cost"
      ],
      "metadata": {
        "id": "AyMxpRp4oQMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EX2.Divide the data into three sets: test, validation, and train.\n",
        "\n",
        "\n",
        
        
        "import numpy as np  # for handling multi-dimensional array operation\n",
        "import pandas as pd  # for reading data from csv\n",
        "from sklearn.model_selection import train_test_split as tts #Split arrays or matrices into random train and test subsets\n",
        "\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "# main method where we train our dataset\n",
        "def init():\n",
        "    \n",
        "    # SVM only takes numbers as input.This indicates that we ought to convert our jpg to csv.\n",
        "    # TODO:We should convert jpg to csv prior to pd.read_csv.\n",
        "    for file in os.listdir(base_dir): #throw every directory\n",
        "      if file.endswith(\".jpeg\"): #find files ending with .jpeg\n",
        "        convert_jpeg_to_csv(file) #call function to cinvert jpeg to csv\n",
        "\n",
       
       
       
       "\n",
                      "    data = pd.read_csv('./data.csv')\n",
        "    # Consequently, the M and B categories will be transformed into\n",
        "    # values 1 and -1 (or -1 and 1), respectively.\n",
        "    collect_map = {'M': 1, 'B': -1}\n",
        "    data['diagnosis'] = data['diagnosis'].map(collect_map)\n",
        "    data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n",
        "\n",
        "    # put features & outputs in different DataFrames for convenience\n",
        "    Y = data.loc[:, 'diagnosis']  # all rows of 'diagnosis'\n",
        "    X = data.iloc[:, 1:]  # all rows of column 1 and ahead (features)\n",
        "\n",
       
       
       "    # dividing the dataset into train and test sets\n",
        "    # it is our dataframes\n",
        "    # tts(*arrays, test_size, train_size=None, train_size, random_state=None, shuffle=True, stratify=None)\n",
        "    X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.1, train_size=0.9, random_state=42)\n",
        "    # W = sgd(X_train.to_numpy(), y_train.to_numpy())\n",
        "    # weights array of given shape and type, filled with zeros.\n",
        "    # shape(n) - return shape of the array \n",
        "    weights = np.zeros(X_train.shape[1])\n",
        "    cost = svm(weights, features, outputs)\n",
        "    print(\"Cost is: {}\".format(cost))\n",
        "    print(\"training finished.\")\n",
        "    print(\"weights are: {}\".format(W))\n",
        "\n",
        "\n",
       
       
       "def sgd(features, outputs):\n",
        "    # Stop training once max number of epochs is reached. In our case is 5000\n",
        "    max_epochs = 5000\n",
        "    # weights array of given shape and type, filled with zeros.\n",
        "    weights = np.zeros(features.shape[1])\n",
        "    nth = 0\n",
        "    prev_cost = float(\"inf\")\n",
        "    cost_threshold = 0.01  # in percent\n",
        "    # stochastic gradient descent\n",
        "    for epoch in range(1, max_epochs):\n",
        "        # shuffle to prevent repeating update cycles\n",
        "        X, Y = shuffle(features, outputs)\n",
        "        for ind, x in enumerate(X):\n",
        "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
        "            weights = weights - (learning_rate * ascent)\n",
        "        # convergence check on 2^nth epoch\n",
        "        if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
        "            cost = compute_cost(weights, features, outputs)\n",
        "            print(\"Epoch is:{} and Cost is: {}\".format(epoch, cost))\n",
        "            # stoppage criterion\n",
        "            if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
        "                return weights\n",
        "            prev_cost = cost\n",
        "            nth += 1\n",
        "    return weights"
      ],
      "metadata": {
        "id": "sf8VWtYC8kml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sotmax Implementation\n",
       
        "#To avoid this problem, we normalize each value  by subtracting the largest value.\n",
        "def softmax(z):\n",
        "    z -= np.max(z)\n",
        "    return np.exp(z) / np.sum(np.exp(z))"
      ],
      "metadata": {
        "id": "mjuGSLEgPqtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ridge_regression(X, y, alpha=0.01, lambda_value=1, epochs=30):\n",
        " \n",
        "    m = np.shape(X)[0]  # total number of samples\n",
        "    n = np.shape(X)[1]  # total number of features\n",
        " \n",
        "    X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
        "    W = np.random.randn(n + 1, )\n",
        " \n",
        "    # stores the updates on the cost function (loss function)\n",
        "    cost_history_list = []\n",
        
        
        " \n",
        "    # iterate until the maximum number of epochs\n",
        "    for current_iteration in range(epochs):  # begin the process\n",
        " \n",
       
        
        "        # compute the dot product between our feature 'X' and weight 'W'\n",
        "        y_estimated = X.dot(W)\n",
        " \n",
        "        error = y_estimated - y\n",
        " \n",
        "        ridge_reg_term = (lambda_value / 2 * m) * np.sum(np.square(W))\n",
        " \n",
        "        cost = (1 / 2 * m) * np.sum(error ** 2) + ridge_reg_term\n",
        " \n",
        "        gradient = (1 / m) * (X.T.dot(error) + (lambda_value * W))\n",
        " \n",
        "        W = W - alpha * gradient\n",
        " \n",
        "        print(f\"cost:{cost} \\t iteration: {current_iteration}\")\n",
        " \n",
        "        cost_history_list.append(cost)\n",
        " \n",
        "    return W, cost_history_list"
      ],
      "metadata": {
        "id": "1UNPvMe-iu3L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
